

from transformers import AutoTokenizer
import torch


_SPECIAL_STR = "######^^^^^^"
class ERCTokenizer:
    """Uses the T5 tokenizer to convert an input for processing.

    For more information, please see the T5 paper, "Exploring the Limits of
    Transfer Learning with a Unified Text-to-Text Transformer".
    Appendix D contains information about the various tasks supported
    by T5.

    Supports the following modes:

    * summarization: summarize English text
    * english_to_german: translate English to German
    * english_to_french: translate English to French
    * english_to_romanian: translate English to Romanian
    """     
    def __init__(self, bert_path,vocab_ids,vocab_map, mode="english_to_german", max_length=64):
        if mode == "english_to_german":
            self.tokenization_prefix = "translate English to German: "
        elif mode == "english_to_french":
            self.tokenization_prefix = "translate English to French: "
        elif mode == "english_to_romanian":
            self.tokenization_prefix = "translate English to Romanian: "
        elif mode == "summarization":
            self.tokenization_prefix = "summarize: "
        else:
            raise ValueError(f"Invalid t5 tokenizer mode {mode}.")

        self.tokenizer = AutoTokenizer.from_pretrained(bert_path)
        self.max_length = max_length
        self.vocab_ids = vocab_ids
        self.vocab_map = vocab_map

    def __call__(self, text, *args, **kwargs):
        """
        Args:
            text (:obj:`str`, :obj:`List[str]`):
                    The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings.
        """
        assert isinstance(text, str) or (
            isinstance(text, (list, tuple))
            and (len(text) == 0 or isinstance(text[0], str))
        ), "`text` must be a string or a list of strings."
        
        #print('text', text)
        if isinstance(text, str):
        #    text = self.tokenization_prefix + text
            return self.parse_and_tokenize(text)
        else:
            res = []
            for text_tmp in text:
                res += [self.parse_and_tokenize(text_tmp)]
            return res
            #return self.parse_and_tokenize(text[0])
        #    for i in range(len(text)):
        #        text[i] = self.tokenization_prefix + text[i]
        
        #return self.parse_and_tokenize(text) #self.tokenizer(text, *args, max_length=self.max_length, **kwargs)

    def decode(self, ids):
        """Converts IDs (typically generated by the model) back to a string."""
        return self.tokenizer.decode(ids)
    
    def parse_and_tokenize(self, text):
        [target, context_intra] = text.split(_SPECIAL_STR)
        #labels = int(labels)
        batch ={}
        batch['tgt_input_ids'] = self.tokenizer(target, add_special_tokens=False, padding=True, return_tensors="pt")
                #print('tgt_input_ids', batch['tgt_input_ids']['input_ids'].shape)
                #print(batch['tgt_input_ids']['input_ids'][0])
                
                #print('target', target)
        #print('context_intra', context_intra)
        '''
        print('tgt_input_ids', batch['tgt_input_ids']['input_ids'].shape)
        print('tgt_input_ids', batch['tgt_input_ids'])
        print("batch['tgt_input_ids']['input_ids'][0]", batch['tgt_input_ids']['input_ids'][0])
        '''
        batch['intra_input_ids'] = self.tokenizer(target, context_intra, add_special_tokens=True, padding=True, return_tensors="pt")
        #print('target', target)
        #print('context_intra', context_intra)
        #print('intra_input_ids', batch['intra_input_ids']['input_ids'].shape)
        #print(batch['intra_input_ids']['input_ids'][0])
        '''
        print("batch['intra_input_ids']",batch['intra_input_ids'])
        print('intra_input_ids', batch['intra_input_ids']['input_ids'].shape)
        print("batch['intra_input_ids']['input_ids'][0]", batch['intra_input_ids']['input_ids'][0])
        '''
        batch['intra'] = self.tokenizer(target, context_intra, padding='longest', truncation='only_second', max_length=300, return_tensors="pt")
        batch['reconstruct_labels'] = self.get_reconstruct_label(batch['intra']['input_ids'], (len(batch['intra']['input_ids']), len(self.vocab_ids)), self.vocab_map)
        batch['distribution_labels'] = self.get_distribution_label(batch['intra']['input_ids'], (len(batch['intra']['input_ids']), len(self.vocab_ids)), self.vocab_map)
        batch['labels'] = torch.tensor(0)
        #rint('reconstruct_labels', batch['reconstruct_labels'])
        #print('reconstruct_labels', batch['reconstruct_labels'].shape)
        
        #print('distribution_labels', batch['distribution_labels'])
        #print('distribution_labels', batch['distribution_labels'].shape)
        #batch['labels'] = torch.tensor(labels)
        return batch
    
    
    def get_distribution_label(self, input_ids, shape, vocab_map):
        oritional = torch.zeros(shape)+(1/len(vocab_map)*100)
        #print('oritional.shape', oritional.shape)
        #print('oritional', oritional)
        #print('input_ids', input_ids.shape)
        #print('vocab_map', len(vocab_map))
        for i, input_id in enumerate(input_ids):
            for id in input_id:
                id = int(id)
                if id in vocab_map:
                    pos = vocab_map[id]
                    oritional[i][pos] += 1
        total = oritional.sum(dim=1).unsqueeze(1)
        #print('total', total)
        #print('before oritional', oritional)
        #print('before oritional', oritional.shape)
        oritional = torch.div(oritional,total)
        #print('torch.div oritional', oritional)
        return oritional 
    
    
    def get_reconstruct_label(self, input_ids, shape, vocab_map):
        oritional = torch.zeros(shape)
        for i, input_id in enumerate(input_ids):
            for id in input_id:
                id = int(id)
                if id in vocab_map:
                    pos = vocab_map[id]
                    oritional[i][pos] = 1
        return oritional 

